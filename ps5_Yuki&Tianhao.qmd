---
title: "PS5"
author: "Yuki Yan & Tianhao Zhang"
date: "Nov7"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Yuki Yan yukiyan
    - Partner 2 (name and cnet ID): Tianhao Zhang
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\Y\Y\*\* \*\*\T\Z\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\0\_\*\* Late coins left after submission: \*\*\1\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime

def fetch_agency_details(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return {'title': 'No Title Found', 'agency': 'No Agency Found', 'date': None}
        
        soup = BeautifulSoup(response.text, 'html.parser')
        result = {'title': 'No Title Found', 'agency': 'No Agency Found', 'date': None}
        
        # Fetch the title using <h1> tag
        title_tag = soup.find('h1', class_='font-heading-xl')
        if title_tag:
            result['title'] = title_tag.text.strip()
        
        # Locate the `ul` and find `li` for agency
        ul = soup.find('ul', class_='usa-list usa-list--unstyled margin-y-2')
        date_tag = soup.find('span', class_='date')  # Example of where the date might be found
        if date_tag:
            result['date'] = pd.to_datetime(date_tag.text, errors='coerce')
        if ul:
            for li in ul.find_all('li'):
                span = li.find('span', class_='padding-right-2 text-base', string='Agency:')
                if span:
                    result['agency'] = li.text.replace('Agency:', '').strip()
                    break
        
        return result
    except requests.RequestException as e:
        return {'title': 'No Title Found', 'agency': f'Failed to fetch due to: {e}', 'date': None}

def fetch_enforcement_links(page_url):
    try:
        response = requests.get(page_url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            if 'href' in a_tag.attrs and '/fraud/enforcement/' in a_tag['href']:
                links.append('https://oig.hhs.gov' + a_tag['href'])
        
        return links
    except requests.RequestException as e:
        print(f"Failed to process {page_url}: {e}")
        return []

def main():
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    data = []
    page_num = 1
    max_pages = 482  # Set the maximum number of pages to process

    while page_num <= max_pages:
        page_url = f"{base_url}?page={page_num}"
        print(f"Processing {page_url}...")
        all_links = fetch_enforcement_links(page_url)
        
        if not all_links:  # If no links are found, it's likely we are at the end of available content
            print(f"No links found on page {page_num}, stopping.")
            break
        
        for link in all_links:
            details = fetch_agency_details(link)
            if details['title'] != 'No Title Found' and details['agency'] != 'No Agency Found':
                data.append(details)
        
        page_num += 1
        time.sleep(1)  # Throttle requests to avoid server overload

    if data:
        df = pd.DataFrame(data)
        df.to_csv('enforcement_actions_agencies.csv', index=False)
        print("Data collected and saved successfully:")
        print(df.head())
    else:
        print("No valid data was collected.")

if __name__ == '__main__':
    main()



```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

def fetch_agency_details(url):
    """Fetch and return the agency name from an enforcement action detail page."""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return 'Failed to retrieve'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        agency_name = "No Agency Found"
        ul = soup.find('ul', class_='usa-list usa-list--unstyled margin-y-2')
        if ul:
            for li in ul.find_all('li'):
                span = li.find('span', class_='padding-right-2 text-base', string='Agency:')
                if span:
                    agency_name = li.text.replace('Agency:', '').strip()
                    break
        return agency_name
    except requests.RequestException as e:
        return f'Failed to fetch due to: {e}'

def scrape_oig_actions(url):
    """Scrapes the OIG website for enforcement actions."""
    data = []
    page_num = 1
    end_date_threshold = datetime(2024, 10, 1)  # Stop processing if the date is after October 2024
    start_date_threshold = datetime(2021, 1, 1)  # Start processing from January 2021

    while True:
        page_url = f"{url}?page={page_num}"
        response = requests.get(page_url)
        if response.status_code != 200:
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

        for action in actions:
            date_span = action.find("span", class_="text-base-dark padding-right-105")
            date = pd.to_datetime(date_span.get_text(strip=True), errors='coerce') if date_span else None

            if date and date < start_date_threshold:
                continue  # Skip actions before January 2021
            if date and date > end_date_threshold:
                return pd.DataFrame(data)  # Stop processing if the date is after October 2024

            title_tag = action.find("h2", class_="usa-card__heading").find("a")
            title = title_tag.get_text(strip=True)
            link = "https://oig.hhs.gov" + title_tag["href"]
            category = action.find("li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1").get_text(strip=True)
            agency = fetch_agency_details(link)

            data.append({
                'Title': title,
                'Date': date,
                'Category': category,
                'Link': link,
                'Agency': agency
            })
        
        page_num += 1
        time.sleep(1)  # Sleep to be respectful to the server

    return pd.DataFrame(data)

# URL for OIG enforcement actions
url = 'https://oig.hhs.gov/fraud/enforcement/'
df = scrape_oig_actions(url)

# Optionally save to CSV
df.to_csv('/mnt/data/enforcement_actions_complete.csv', index=False)

print(df.head())  # Print the first few rows to verify data

```
